{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7IXUfiQ2UKj6"
   },
   "source": [
    "Lambda School Data Science, Unit 2: Predictive Modeling\n",
    "\n",
    "# Kaggle Challenge, Module 3\n",
    "\n",
    "## Assignment\n",
    "- [ ] [Review requirements for your portfolio project](https://lambdaschool.github.io/ds/unit2/portfolio-project/ds6), then choose your dataset, and [submit this form](https://forms.gle/nyWURUg65x1UTRNV9), due today at 4pm Pacific.\n",
    "- [ ] Continue to participate in our Kaggle challenge.\n",
    "- [ ] Try xgboost.\n",
    "- [ ] Get your model's permutation importances.\n",
    "- [ ] Try feature selection with permutation importances.\n",
    "- [ ] Submit your predictions to our Kaggle competition. (Go to our Kaggle InClass competition webpage. Use the blue **Submit Predictions** button to upload your CSV file. Or you can use the Kaggle API to submit your predictions.)\n",
    "- [ ] Commit your notebook to your fork of the GitHub repo.\n",
    "\n",
    "## Stretch Goals\n",
    "\n",
    "### Doing\n",
    "- [ ] Add your own stretch goal(s) !\n",
    "- [ ] Do more exploratory data analysis, data cleaning, feature engineering, and feature selection.\n",
    "- [ ] Try other categorical encodings.\n",
    "- [ ] Try other Python libraries for gradient boosting.\n",
    "- [ ] Look at the bonus notebook in the repo, about monotonic constraints with gradient boosting.\n",
    "- [ ] Make visualizations and share on Slack.\n",
    "\n",
    "### Reading\n",
    "\n",
    "Top recommendations in _**bold italic:**_\n",
    "\n",
    "#### Permutation Importances\n",
    "- _**[Kaggle / Dan Becker: Machine Learning Explainability](https://www.kaggle.com/dansbecker/permutation-importance)**_\n",
    "- [Christoph Molnar: Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/feature-importance.html)\n",
    "\n",
    "#### (Default) Feature Importances\n",
    "  - [Ando Saabas: Selecting good features, Part 3, Random Forests](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/)\n",
    "  - [Terence Parr, et al: Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)\n",
    "\n",
    "#### Gradient Boosting\n",
    "  - [A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)\n",
    "  - _**[A Kaggle Master Explains Gradient Boosting](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)**_\n",
    "  - [_An Introduction to Statistical Learning_](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf) Chapter 8\n",
    "  - [Gradient Boosting Explained](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)\n",
    "  - _**[Boosting](https://www.youtube.com/watch?v=GM3CDQfQ4sw) (2.5 minute video)**_\n",
    "\n",
    "#### Categorical encoding for trees\n",
    "- [Are categorical variables getting lost in your random forests?](https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/)\n",
    "- [Beyond One-Hot: An Exploration of Categorical Variables](http://www.willmcginnis.com/2015/11/29/beyond-one-hot-an-exploration-of-categorical-variables/)\n",
    "- _**[Categorical Features and Encoding in Decision Trees](https://medium.com/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931)**_\n",
    "- _**[Coursera — How to Win a Data Science Competition: Learn from Top Kagglers — Concept of mean encoding](https://www.coursera.org/lecture/competitive-data-science/concept-of-mean-encoding-b5Gxv)**_\n",
    "- [Mean (likelihood) encodings: a comprehensive study](https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study)\n",
    "- [The Mechanics of Machine Learning, Chapter 6: Categorically Speaking](https://mlbook.explained.ai/catvars.html)\n",
    "\n",
    "#### Imposter Syndrome\n",
    "- [Effort Shock and Reward Shock (How The Karate Kid Ruined The Modern World)](http://www.tempobook.com/2014/07/09/effort-shock-and-reward-shock/)\n",
    "- [How to manage impostor syndrome in data science](https://towardsdatascience.com/how-to-manage-impostor-syndrome-in-data-science-ad814809f068)\n",
    "- [\"I am not a real data scientist\"](https://brohrer.github.io/imposter_syndrome.html)\n",
    "- _**[Imposter Syndrome in Data Science](https://caitlinhudon.com/2018/01/19/imposter-syndrome-in-data-science/)**_\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wd6T-m6bneLS"
   },
   "source": [
    "### Python libraries for Gradient Boosting\n",
    "- [scikit-learn Gradient Tree Boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting) — slower than other libraries, but [the new version may be better](https://twitter.com/amuellerml/status/1129443826945396737)\n",
    "  - Anaconda: already installed\n",
    "  - Google Colab: already installed\n",
    "- [xgboost](https://xgboost.readthedocs.io/en/latest/) — can accept missing values and enforce [monotonic constraints](https://xiaoxiaowang87.github.io/monotonicity_constraint/)\n",
    "  - Anaconda, Mac/Linux: `conda install -c conda-forge xgboost`\n",
    "  - Windows: `conda install -c anaconda py-xgboost`\n",
    "  - Google Colab: already installed\n",
    "- [LightGBM](https://lightgbm.readthedocs.io/en/latest/) — can accept missing values and enforce [monotonic constraints](https://blog.datadive.net/monotonicity-constraints-in-machine-learning/)\n",
    "  - Anaconda: `conda install -c conda-forge lightgbm`\n",
    "  - Google Colab: already installed\n",
    "- [CatBoost](https://catboost.ai/) — can accept missing values and use [categorical features](https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html) without preprocessing\n",
    "  - Anaconda: `conda install -c conda-forge catboost`\n",
    "  - Google Colab: `pip install catboost`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o1nF5eU9nJwL"
   },
   "source": [
    "### Categorical Encodings\n",
    "\n",
    "**1.** The article **[Categorical Features and Encoding in Decision Trees](https://medium.com/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931)** mentions 4 encodings:\n",
    "\n",
    "- **\"Categorical Encoding\":** This means using the raw categorical values as-is, not encoded. Scikit-learn doesn't support this, but some tree algorithm implementations do. For example, [Catboost](https://catboost.ai/), or R's [rpart](https://cran.r-project.org/web/packages/rpart/index.html) package.\n",
    "- **Numeric Encoding:** Synonymous with Label Encoding, or \"Ordinal\" Encoding with random order. We can use [category_encoders.OrdinalEncoder](https://contrib.scikit-learn.org/categorical-encoding/ordinal.html).\n",
    "- **One-Hot Encoding:** We can use [category_encoders.OneHotEncoder](http://contrib.scikit-learn.org/categorical-encoding/onehot.html).\n",
    "- **Binary Encoding:** We can use [category_encoders.BinaryEncoder](http://contrib.scikit-learn.org/categorical-encoding/binary.html).\n",
    "\n",
    "\n",
    "**2.** The short video \n",
    "**[Coursera — How to Win a Data Science Competition: Learn from Top Kagglers — Concept of mean encoding](https://www.coursera.org/lecture/competitive-data-science/concept-of-mean-encoding-b5Gxv)** introduces an interesting idea: use both X _and_ y to encode categoricals.\n",
    "\n",
    "Category Encoders has multiple implementations of this general concept:\n",
    "\n",
    "- [CatBoost Encoder](http://contrib.scikit-learn.org/categorical-encoding/catboost.html)\n",
    "- [James-Stein Encoder](http://contrib.scikit-learn.org/categorical-encoding/jamesstein.html)\n",
    "- [Leave One Out](http://contrib.scikit-learn.org/categorical-encoding/leaveoneout.html)\n",
    "- [M-estimate](http://contrib.scikit-learn.org/categorical-encoding/mestimate.html)\n",
    "- [Target Encoder](http://contrib.scikit-learn.org/categorical-encoding/targetencoder.html)\n",
    "- [Weight of Evidence](http://contrib.scikit-learn.org/categorical-encoding/woe.html)\n",
    "\n",
    "Category Encoder's mean encoding implementations work for regression problems or binary classification problems. \n",
    "\n",
    "For multi-class classification problems, you will need to temporarily reformulate it as binary classification. For example:\n",
    "\n",
    "```python\n",
    "encoder = ce.TargetEncoder(min_samples_leaf=..., smoothing=...) # Both parameters > 1 to avoid overfitting\n",
    "X_train_encoded = encoder.fit_transform(X_train, y_train=='functional')\n",
    "X_val_encoded = encoder.transform(X_train, y_val=='functional')\n",
    "```\n",
    "\n",
    "**3.** The **[dirty_cat](https://dirty-cat.github.io/stable/)** library has a Target Encoder implementation that works with multi-class classification.\n",
    "\n",
    "```python\n",
    " dirty_cat.TargetEncoder(clf_type='multiclass-clf')\n",
    "```\n",
    "It also implements an interesting idea called [\"Similarity Encoder\" for dirty categories](https://www.slideshare.net/GaelVaroquaux/machine-learning-on-non-curated-data-154905090).\n",
    "\n",
    "However, it seems like dirty_cat doesn't handle missing values or unknown categories as well as category_encoders does. And you may need to use it with one column at a time, instead of with your whole dataframe.\n",
    "\n",
    "**4. [Embeddings](https://www.kaggle.com/learn/embeddings)** can work well with sparse / high cardinality categoricals.\n",
    "\n",
    "_**I hope it’s not too frustrating or confusing that there’s not one “canonical” way to encode categorcals. It’s an active area of research and experimentation! Maybe you can make your own contributions!**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o9eSnDYhUGD7"
   },
   "outputs": [],
   "source": [
    "# If you're in Colab...\n",
    "import os, sys\n",
    "in_colab = 'google.colab' in sys.modules\n",
    "\n",
    "if in_colab:\n",
    "    # Install required python packages:\n",
    "    # category_encoders, version >= 2.0\n",
    "    # eli5, version >= 0.9\n",
    "    # pandas-profiling, version >= 2.0\n",
    "    # plotly, version >= 4.0\n",
    "    !pip install --upgrade category_encoders eli5 pandas-profiling plotly\n",
    "    \n",
    "    # Pull files from Github repo\n",
    "    os.chdir('/content')\n",
    "    !git init .\n",
    "    !git remote add origin https://github.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge.git\n",
    "    !git pull origin master\n",
    "    \n",
    "    # Change into directory for module\n",
    "    os.chdir('module3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QJBD4ruICm1m"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 41), (14358, 40), (14358, 2))"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train = pd.merge(pd.read_csv('../data/tanzania/train_features.csv'), \n",
    "                 pd.read_csv('../data/tanzania/train_labels.csv'))\n",
    "test = pd.read_csv('../data/tanzania/test_features.csv')\n",
    "sample_submission = pd.read_csv('../data/tanzania/sample_submission.csv')\n",
    "\n",
    "train.shape, test.shape, sample_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a list of the top 10 neighborhoods\n",
    "top50_scheme = train['scheme_name'].value_counts()[:150].index\n",
    "top50_wpt_name = train['wpt_name'].value_counts()[:150].index\n",
    "top50_funder= train['funder'].value_counts()[:100].index\n",
    "top50_installer= train['installer'].value_counts()[:100].index\n",
    "top50_subvillage = train['subvillage'].value_counts()[:100].index\n",
    "top50_ward= train['ward'].value_counts()[:100].index\n",
    "\n",
    "# # train['scheme_name'].value_counts()[:50].index\n",
    "# # At locations where the neighborhood is NOT in the top 10,\n",
    "# # replace the neighborhood with 'OTHER'\n",
    "train.loc[~train['scheme_name'].isin(top50_scheme), 'scheme_name'] = 'OTHER'\n",
    "test.loc[~test['scheme_name'].isin(top50_scheme), 'scheme_name'] = 'OTHER'\n",
    "\n",
    "train.loc[~train['wpt_name'].isin(top50_wpt_name), 'wpt_name'] = 'OTHER'\n",
    "test.loc[~test['wpt_name'].isin(top50_wpt_name), 'wpt_name'] = 'OTHER'\n",
    "\n",
    "train.loc[~train['funder'].isin(top50_funder), 'funder'] = 'OTHER'\n",
    "test.loc[~test['funder'].isin(top50_funder), 'funder'] = 'OTHER'\n",
    "\n",
    "train.loc[~train['installer'].isin(top50_installer), 'installer'] = 'OTHER'\n",
    "test.loc[~test['installer'].isin(top50_installer), 'installer'] = 'OTHER'\n",
    "\n",
    "train.loc[~train['subvillage'].isin(top50_subvillage), 'subvillage'] = 'OTHER'\n",
    "test.loc[~test['subvillage'].isin(top50_subvillage), 'subvillage'] = 'OTHER'\n",
    "\n",
    "train.loc[~train['ward'].isin(top50_ward), 'ward'] = 'OTHER'\n",
    "test.loc[~test['ward'].isin(top50_ward), 'ward'] = 'OTHER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>recorded_by</th>\n",
       "      <td>59400</td>\n",
       "      <td>1</td>\n",
       "      <td>GeoData Consultants Ltd</td>\n",
       "      <td>59400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public_meeting</th>\n",
       "      <td>56066</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>51011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>permit</th>\n",
       "      <td>56344</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>38852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>status_group</th>\n",
       "      <td>59400</td>\n",
       "      <td>3</td>\n",
       "      <td>functional</td>\n",
       "      <td>32259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_class</th>\n",
       "      <td>59400</td>\n",
       "      <td>3</td>\n",
       "      <td>groundwater</td>\n",
       "      <td>45794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantity_group</th>\n",
       "      <td>59400</td>\n",
       "      <td>5</td>\n",
       "      <td>enough</td>\n",
       "      <td>33186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantity</th>\n",
       "      <td>59400</td>\n",
       "      <td>5</td>\n",
       "      <td>enough</td>\n",
       "      <td>33186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>management_group</th>\n",
       "      <td>59400</td>\n",
       "      <td>5</td>\n",
       "      <td>user-group</td>\n",
       "      <td>52490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality_group</th>\n",
       "      <td>59400</td>\n",
       "      <td>6</td>\n",
       "      <td>good</td>\n",
       "      <td>50818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waterpoint_type_group</th>\n",
       "      <td>59400</td>\n",
       "      <td>6</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>34625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waterpoint_type</th>\n",
       "      <td>59400</td>\n",
       "      <td>7</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>28522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>payment_type</th>\n",
       "      <td>59400</td>\n",
       "      <td>7</td>\n",
       "      <td>never pay</td>\n",
       "      <td>25348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extraction_type_class</th>\n",
       "      <td>59400</td>\n",
       "      <td>7</td>\n",
       "      <td>gravity</td>\n",
       "      <td>26780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_type</th>\n",
       "      <td>59400</td>\n",
       "      <td>7</td>\n",
       "      <td>spring</td>\n",
       "      <td>17021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>payment</th>\n",
       "      <td>59400</td>\n",
       "      <td>7</td>\n",
       "      <td>never pay</td>\n",
       "      <td>25348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water_quality</th>\n",
       "      <td>59400</td>\n",
       "      <td>8</td>\n",
       "      <td>soft</td>\n",
       "      <td>50818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basin</th>\n",
       "      <td>59400</td>\n",
       "      <td>9</td>\n",
       "      <td>Lake Victoria</td>\n",
       "      <td>10248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <td>59400</td>\n",
       "      <td>10</td>\n",
       "      <td>spring</td>\n",
       "      <td>17021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scheme_management</th>\n",
       "      <td>55523</td>\n",
       "      <td>12</td>\n",
       "      <td>VWC</td>\n",
       "      <td>36793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>management</th>\n",
       "      <td>59400</td>\n",
       "      <td>12</td>\n",
       "      <td>vwc</td>\n",
       "      <td>40507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extraction_type_group</th>\n",
       "      <td>59400</td>\n",
       "      <td>13</td>\n",
       "      <td>gravity</td>\n",
       "      <td>26780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extraction_type</th>\n",
       "      <td>59400</td>\n",
       "      <td>18</td>\n",
       "      <td>gravity</td>\n",
       "      <td>26780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region</th>\n",
       "      <td>59400</td>\n",
       "      <td>21</td>\n",
       "      <td>Iringa</td>\n",
       "      <td>5294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ward</th>\n",
       "      <td>59400</td>\n",
       "      <td>101</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>47586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subvillage</th>\n",
       "      <td>59400</td>\n",
       "      <td>101</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>50619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>installer</th>\n",
       "      <td>59400</td>\n",
       "      <td>101</td>\n",
       "      <td>DWE</td>\n",
       "      <td>17402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funder</th>\n",
       "      <td>59400</td>\n",
       "      <td>101</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>15223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lga</th>\n",
       "      <td>59400</td>\n",
       "      <td>125</td>\n",
       "      <td>Njombe</td>\n",
       "      <td>2503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scheme_name</th>\n",
       "      <td>59400</td>\n",
       "      <td>151</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>45594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wpt_name</th>\n",
       "      <td>59400</td>\n",
       "      <td>151</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>46044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_recorded</th>\n",
       "      <td>59400</td>\n",
       "      <td>356</td>\n",
       "      <td>2011-03-15</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       count unique                      top   freq\n",
       "recorded_by            59400      1  GeoData Consultants Ltd  59400\n",
       "public_meeting         56066      2                     True  51011\n",
       "permit                 56344      2                     True  38852\n",
       "status_group           59400      3               functional  32259\n",
       "source_class           59400      3              groundwater  45794\n",
       "quantity_group         59400      5                   enough  33186\n",
       "quantity               59400      5                   enough  33186\n",
       "management_group       59400      5               user-group  52490\n",
       "quality_group          59400      6                     good  50818\n",
       "waterpoint_type_group  59400      6       communal standpipe  34625\n",
       "waterpoint_type        59400      7       communal standpipe  28522\n",
       "payment_type           59400      7                never pay  25348\n",
       "extraction_type_class  59400      7                  gravity  26780\n",
       "source_type            59400      7                   spring  17021\n",
       "payment                59400      7                never pay  25348\n",
       "water_quality          59400      8                     soft  50818\n",
       "basin                  59400      9            Lake Victoria  10248\n",
       "source                 59400     10                   spring  17021\n",
       "scheme_management      55523     12                      VWC  36793\n",
       "management             59400     12                      vwc  40507\n",
       "extraction_type_group  59400     13                  gravity  26780\n",
       "extraction_type        59400     18                  gravity  26780\n",
       "region                 59400     21                   Iringa   5294\n",
       "ward                   59400    101                    OTHER  47586\n",
       "subvillage             59400    101                    OTHER  50619\n",
       "installer              59400    101                      DWE  17402\n",
       "funder                 59400    101                    OTHER  15223\n",
       "lga                    59400    125                   Njombe   2503\n",
       "scheme_name            59400    151                    OTHER  45594\n",
       "wpt_name               59400    151                    OTHER  46044\n",
       "date_recorded          59400    356               2011-03-15    572"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe(exclude='number').T.sort_values(by='unique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((47520, 41), (11880, 41))"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, val = train_test_split(train, train_size=0.80, test_size=0.20,  stratify=train['status_group'],\n",
    "                             random_state=42)\n",
    "train.shape, val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def wrangle(X): #clean messy data\n",
    "#     X = X.copy()\n",
    "    \n",
    "#     # About 3% of the time, latitude has small values near zero,\n",
    "#     # outside Tanzania, so we'll treat these values like zero.\n",
    "#     X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
    "    \n",
    "#     # When columns have zeros and shouldn't, they are like null values.\n",
    "#     # So we will replace the zeros with nulls, and impute missing values later.\n",
    "#     cols_with_zeros = ['longitude', 'latitude','construction_year', \n",
    "#                        'gps_height', 'population']\n",
    "#     for col in cols_with_zeros:\n",
    "#         X[col] = X[col].replace(0, np.nan)\n",
    "#         X[col+'_MISSING'] = X[col].isnull()\n",
    "            \n",
    "#     # quantity & quantity_group are duplicates, so drop one\n",
    "#     # payment, peyment_type, are duplicates, so drop one\n",
    "\n",
    "#     # extraction_type_group, extraction_type_class, extraction_type, so drop one\n",
    "# #     X = X.drop(columns=['quantity_group','payment_type','extraction_type_group',\n",
    "# #                         'extraction_type_class','waterpoint_type_group'])\n",
    "    \n",
    "#     X = X.drop(columns=['extraction_type','extraction_type_group','management','payment',\n",
    "#                          'water_quality','quantity_group','source','waterpoint_type','scheme_management'])\n",
    "         \n",
    "#     #Change format date\n",
    "#     X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
    "    \n",
    "#     # Extract components from date_recorded, then drop the original column\n",
    "#     X['year_recorded'] = X['date_recorded'].dt.year\n",
    "#     X['month_recorded'] = X['date_recorded'].dt.month\n",
    "#     X['day_recorded'] = X['date_recorded'].dt.day\n",
    "#     X = X.drop(columns='date_recorded')\n",
    "    \n",
    "#     #Calculate year instalation pump\n",
    "# #     X['years_instalation'] = X.date_recorded.dt.year - X.construction_year\n",
    "# #     X['years_instalation'] = X['years_instalation'].replace([-9,-8,-7, -6, -5, -4, -3, -2, -1], np.nan)\n",
    "    \n",
    "#     X['years'] = X['year_recorded'] - X['construction_year']\n",
    "#     X['years_MISSING'] = X['years'].isnull()\n",
    "    \n",
    "    \n",
    "#     #construction year by nan\n",
    "#     X['construction_year'] = X['construction_year'].replace(0, np.nan)\n",
    "    \n",
    "\n",
    "#     # Engineer Feature: is water quality_group is  good and quantity is enough\\n\",\n",
    "#     X['good_quality'] = (X['quality_group'] == 'good').astype(int)\n",
    "#     X['enough_quantity'] = (X['quantity'] == 'enough').astype(int)\n",
    "#     X['good_enough'] = ((X['quality_group'] == 'good')|(X['quantity'] == 'enough')).astype(int)\n",
    "    \n",
    "#     #change region_code to string \n",
    "#     X['region_code'] = (X['region_code']).astype(int)\n",
    "    \n",
    "#     X['x'] = np.cos(X['latitude']) * np.cos(X['longitude'])\n",
    "#     X['y'] = np.cos(X['latitude']) * np.sin(X['longitude'])\n",
    "#     X['z'] = np.sin(X['latitude'])\n",
    "    \n",
    "#     # return the wrangled dataframe\n",
    "#     return X\n",
    "    \n",
    "# train = wrangle(train)\n",
    "# val = wrangle(val)\n",
    "# test = wrangle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def wrangle(X): #clean messy data\n",
    "    X = X.copy()\n",
    "    \n",
    "    # About 3% of the time, latitude has small values near zero,\n",
    "    # outside Tanzania, so we'll treat these values like zero.\n",
    "    X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
    "    \n",
    "    # When columns have zeros and shouldn't, they are like null values.\n",
    "    # So we will replace the zeros with nulls, and impute missing values later.\n",
    "    cols_with_zeros = ['longitude', 'latitude','construction_year', \n",
    "                       'population']\n",
    "    for col in cols_with_zeros:\n",
    "        X[col] = X[col].replace(0, np.nan)\n",
    "        X[col+'_MISSING'] = X[col].isnull()\n",
    "     \n",
    "    #GPS height\n",
    "    X['gps_height'] = X['gps_height'].replace(0, X['gps_height'].mean() ) \n",
    "    # quantity & quantity_group are duplicates, so drop one\n",
    "    # payment, peyment_type, are duplicates, so drop one\n",
    "\n",
    "    # extraction_type_group, extraction_type_class, extraction_type, so drop one\n",
    "#     X = X.drop(columns=['quantity_group','payment_type','extraction_type_group',\n",
    "#                         'extraction_type_class','waterpoint_type_group'])\n",
    "    \n",
    "    X = X.drop(columns=['extraction_type','extraction_type_group','management','payment',\n",
    "                         'water_quality','quantity_group','source','waterpoint_type','scheme_management'])\n",
    "         \n",
    "    #Change format date\n",
    "    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
    "    \n",
    "    min_recorded = X['date_recorded'].min()\n",
    "    \n",
    "    # Extract components from date_recorded, then drop the original column\n",
    "    X['year_recorded'] = X['date_recorded'].dt.year\n",
    "    X['month_recorded'] = X['date_recorded'].dt.month\n",
    "    X['day_recorded'] = X['date_recorded'].dt.day\n",
    "    \n",
    "    X['years'] = X['year_recorded'] - X['construction_year']\n",
    "    \n",
    "    X['days'] = X['day_recorded'] - X['date_recorded'].dt.day.min()\n",
    "    \n",
    "    X['years_MISSING'] = X['years'].isnull()\n",
    "    \n",
    "    \n",
    "    X = X.drop(columns='date_recorded')\n",
    "    \n",
    "    #Calculate year instalation pump\n",
    "#     X['years_instalation'] = X.date_recorded.dt.year - X.construction_year\n",
    "#     X['years_instalation'] = X['years_instalation'].replace([-9,-8,-7, -6, -5, -4, -3, -2, -1], np.nan)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #construction year,day by nan\n",
    "    X['construction_year'] = X['construction_year'].replace(0, np.nan)\n",
    "    \n",
    "\n",
    "    # Engineer Feature: is water quality_group is  good and quantity is enough\\n\",\n",
    "    X['good_quality'] = (X['quality_group'] == 'good').astype(int)\n",
    "    X['enough_quantity'] = (X['quantity'] == 'enough').astype(int)\n",
    "    X['good_enough'] = ((X['quality_group'] == 'good')|(X['quantity'] == 'enough')).astype(int)\n",
    "    \n",
    "    #change region_code to string \n",
    "    X['region_code'] = (X['region_code']).astype(int)\n",
    "    \n",
    "    \n",
    "    #http://mypages.iit.edu/~maslanka/3Dcoordinates.pdf\n",
    "    \n",
    "    X['x'] =3960*( np.cos(X['latitude']) * np.cos(X['longitude']))\n",
    "    X['y'] = 3960*(np.cos(X['latitude']) * np.sin(X['longitude']))\n",
    "    X['z'] = 3960*(np.sin(X['latitude'])\n",
    "    )\n",
    "    # return the wrangled dataframe\n",
    "    return X\n",
    "    \n",
    "train = wrangle(train)\n",
    "val = wrangle(val)\n",
    "test = wrangle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amount_tsh', 'gps_height', 'longitude', 'latitude', 'num_private', 'region_code', 'district_code', 'population', 'construction_year', 'year_recorded', 'month_recorded', 'day_recorded', 'years', 'days', 'good_quality', 'enough_quantity', 'good_enough', 'x', 'y', 'z', 'funder', 'installer', 'wpt_name', 'basin', 'subvillage', 'region', 'lga', 'ward', 'public_meeting', 'recorded_by', 'scheme_name', 'permit', 'extraction_type_class', 'management_group', 'payment_type', 'quality_group', 'quantity', 'source_type', 'source_class', 'waterpoint_type_group', 'longitude_MISSING', 'latitude_MISSING', 'construction_year_MISSING', 'population_MISSING', 'years_MISSING']\n"
     ]
    }
   ],
   "source": [
    "# The status_group column is the target\n",
    "target = 'status_group'\n",
    "\n",
    "# Get a dataframe with all train columns except the target & id\n",
    "train_features = train.drop(columns=[target, 'id'])\n",
    "\n",
    "# Get a list of the numeric features\n",
    "numeric_features = train_features.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "# Get a series with the cardinality of the nonnumeric features\n",
    "cardinality = train_features.select_dtypes(exclude='number').nunique()\n",
    "\n",
    "# Get a list of all categorical features with cardinality <= 50\n",
    "categorical_features = cardinality[cardinality <= 360].index.tolist()\n",
    "\n",
    "# Combine the lists \n",
    "features = numeric_features + categorical_features\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The status_group column is the target\n",
    "# target = 'status_group'\n",
    "\n",
    "# # Get a dataframe with all train columns except the target & id\n",
    "# train_features = train.drop(columns=[target, 'id'])\n",
    "\n",
    "# # Get a list of the numeric features\n",
    "# numeric_features = train_features.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "# # Get a series with the cardinality of the nonnumeric features\n",
    "# cardinality = train_features.select_dtypes(exclude='number').nunique().index.tolist()\n",
    "\n",
    "# # # Get a list of all categorical features with cardinality <= 50\n",
    "# # categorical_features = cardinality[cardinality <= 360].index.tolist()\n",
    "\n",
    "# # Combine the lists \n",
    "# features = numeric_features + cardinality\n",
    "# print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange data into X features matrix and y target vector \n",
    "X_train = train[features]\n",
    "y_train = train[target]\n",
    "X_val = val[features]\n",
    "y_val = val[target]\n",
    "X_test = test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.9221590909090909\n",
      "Validation Accuracy 0.8148148148148148\n"
     ]
    }
   ],
   "source": [
    "#Option 9\n",
    "#0.8154882154882155\n",
    "# %%time\n",
    "\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    SimpleImputer(strategy='mean'), \n",
    "#     StandardScaler(), \n",
    "    RandomForestClassifier(n_estimators=350, min_samples_leaf=3, n_jobs=-1, random_state=42, \n",
    "                          max_features=12,oob_score=True) \n",
    "    #Max_features features to consider when looking for the best split\n",
    "    #oob_score yes or nor uses samples to estimate the generalization accuracy.\n",
    "    #min_samples_leaf minimum number of samples required to be at a leaf node.\n",
    ")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print('Train Accuracy', pipeline.score(X_train, y_train))\n",
    "print('Validation Accuracy', pipeline.score(X_val, y_val))\n",
    "# Predict on test\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = sample_submission.copy()\n",
    "submission['status_group'] = y_pred\n",
    "submission.to_csv('strake_11.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.921969696969697\n",
      "Validation Accuracy 0.8134680134680135\n"
     ]
    }
   ],
   "source": [
    "#Option 9\n",
    "#0.8154882154882155\n",
    "# %%time\n",
    "\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    SimpleImputer(strategy='most_frequent'), \n",
    "#     StandardScaler(), \n",
    "    RandomForestClassifier(n_estimators=350, min_samples_leaf=3, n_jobs=-1, random_state=42, \n",
    "                          max_features=12,oob_score=True) \n",
    "    #Max_features features to consider when looking for the best split\n",
    "    #oob_score yes or nor uses samples to estimate the generalization accuracy.\n",
    "    #min_samples_leaf minimum number of samples required to be at a leaf node.\n",
    ")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print('Train Accuracy', pipeline.score(X_train, y_train))\n",
    "print('Validation Accuracy', pipeline.score(X_val, y_val))\n",
    "# Predict on test\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.9221590909090909\n",
      "Validation Accuracy 0.8148148148148148\n"
     ]
    }
   ],
   "source": [
    "#Option 9\n",
    "#0.8154882154882155\n",
    "# %%time\n",
    "\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    SimpleImputer(strategy='mean'), \n",
    "#     StandardScaler(), \n",
    "    RandomForestClassifier(n_estimators=350, min_samples_leaf=3, n_jobs=-1, random_state=42, \n",
    "                          max_features=12,oob_score=True) \n",
    "    #Max_features features to consider when looking for the best split\n",
    "    #oob_score yes or nor uses samples to estimate the generalization accuracy.\n",
    "    #min_samples_leaf minimum number of samples required to be at a leaf node.\n",
    ")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print('Train Accuracy', pipeline.score(X_train, y_train))\n",
    "print('Validation Accuracy', pipeline.score(X_val, y_val))\n",
    "# Predict on test\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.922053872053872\n",
      "Validation Accuracy 0.8134680134680135\n"
     ]
    }
   ],
   "source": [
    "#Option 9\n",
    "#0.8154882154882155\n",
    "# %%time\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    SimpleImputer(strategy='median'), \n",
    "    RobustScaler(),#new\n",
    "#     StandardScaler(), \n",
    "    RandomForestClassifier(n_estimators=350, min_samples_leaf=3, n_jobs=-1, random_state=42, \n",
    "                          max_features=12,oob_score=True) \n",
    "    #Max_features features to consider when looking for the best split\n",
    "    #oob_score yes or nor uses samples to estimate the generalization accuracy.\n",
    "    #min_samples_leaf minimum number of samples required to be at a leaf node.\n",
    ")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print('Train Accuracy', pipeline.score(X_train, y_train))\n",
    "print('Validation Accuracy', pipeline.score(X_val, y_val))\n",
    "# Predict on test\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.9222853535353536\n",
      "Validation Accuracy 0.8135521885521886\n"
     ]
    }
   ],
   "source": [
    "#Option 9\n",
    "#0.8154882154882155\n",
    "# %%time\n",
    "\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    ce.TargetEncoder(min_samples_leaf=1, smoothing=1),\n",
    "    SimpleImputer(strategy='median'), \n",
    "#     StandardScaler(), \n",
    "    RandomForestClassifier(n_estimators=350, min_samples_leaf=3, n_jobs=-1, random_state=42, \n",
    "                          max_features=12,oob_score=True) \n",
    "    #Max_features features to consider when looking for the best split\n",
    "    #oob_score yes or nor uses samples to estimate the generalization accuracy.\n",
    "    #min_samples_leaf minimum number of samples required to be at a leaf node.\n",
    ")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print('Train Accuracy', pipeline.score(X_train, y_train))\n",
    "print('Validation Accuracy', pipeline.score(X_val, y_val))\n",
    "# Predict on test\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation:\n",
    "\n",
    "\n",
    "\n",
    "# k = 3\n",
    "# scores = cross_val_score(pipeline, X_train, y_train, cv=k, \n",
    "#                          scoring='neg_mean_absolute_error')\n",
    "# print(f'MAE for {k} folds:', -scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   16.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   35.5s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed: 16.3min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed: 19.5min\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed: 21.4min\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed: 23.4min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 26.5min\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed: 27.8min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 31.8min\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed: 37.8min\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed: 42.5min\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed: 45.1min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed: 48.4min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 52.2min finished\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    ce.TargetEncoder(),\n",
    "    SimpleImputer(), \n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "param_distributions = {\n",
    "    'targetencoder__min_samples_leaf':[1,2,3,4],\n",
    "    'simpleimputer__strategy': ['mean', 'median'], \n",
    "    'randomforestclassifier__n_estimators': randint(0, 350, 20), \n",
    "    'randomforestclassifier__max_depth': [5, 10, 15, 20,None], \n",
    "    'randomforestclassifier__max_features': uniform(0, 1)\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline, \n",
    "    param_distributions=param_distributions, \n",
    "    n_iter=60, \n",
    "    cv=5, \n",
    "    scoring='accuracy', \n",
    "    verbose=10, \n",
    "    return_train_score=True, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters {'randomforestclassifier__max_depth': 20, 'randomforestclassifier__max_features': 0.18022125479724593, 'randomforestclassifier__n_estimators': 293, 'simpleimputer__strategy': 'mean', 'targetencoder__min_samples_leaf': 3}\n",
      "Cross-validation MAE -0.8064604377104377\n"
     ]
    }
   ],
   "source": [
    "print('Best hyperparameters', search.best_params_)\n",
    "print('Cross-validation MAE', -search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.9681397306397307\n",
      "Validation Accuracy 0.8169191919191919\n"
     ]
    }
   ],
   "source": [
    "#este tuvo el mejor \n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    ce.TargetEncoder(min_samples_leaf=3, smoothing=1),\n",
    "    SimpleImputer(strategy='mean'), \n",
    "    RobustScaler(),\n",
    "    RandomForestClassifier(n_estimators=275, n_jobs=-1, random_state=42, \n",
    "                          max_depth=20,oob_score=True,max_features=0.307) \n",
    "    #Max_features features to consider when looking for the best split\n",
    "    #oob_score yes or nor uses samples to estimate the generalization accuracy.\n",
    "    #min_samples_leaf minimum number of samples required to be at a leaf node.\n",
    ")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print('Train Accuracy', pipeline.score(X_train, y_train))\n",
    "print('Validation Accuracy', pipeline.score(X_val, y_val))\n",
    "# Predict on test\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.751830808080808\n",
      "Validation Accuracy 0.744023569023569\n"
     ]
    }
   ],
   "source": [
    "#Option 9\n",
    "#0.8058922558922559\n",
    "# %%time\n",
    "\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    RobustScaler(),\n",
    "    ce.TargetEncoder(min_samples_leaf=1, smoothing=1),\n",
    "    SimpleImputer(strategy='median'), \n",
    "#     StandardScaler(), \n",
    "     XGBClassifier(booster='gbtree',\n",
    "                   n_estimators=150, max_depth=6, learning_rate=0.01, n_jobs=-1,\n",
    "                   min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                   objective= 'binary:logistic', nthread=4, scale_pos_weight=1,cv=5)\n",
    "    #Max_features features to consider when looking for the best split\n",
    "    #oob_score yes or nor uses samples to estimate the generalization accuracy.\n",
    "    #min_samples_leaf minimum number of samples required to be at a leaf node.\n",
    ")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print('Train Accuracy', pipeline.score(X_train, y_train))\n",
    "print('Validation Accuracy', pipeline.score(X_val, y_val))\n",
    "# Predict on test\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = sample_submission.copy()\n",
    "submission['status_group'] = y_pred\n",
    "submission.to_csv('strake_11.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    ce.TargetEncoder(),\n",
    "    SimpleImputer(), \n",
    "    XGBClassifier()\n",
    ")\n",
    "\n",
    "param_distributions = {\n",
    "    'targetencoder__min_samples_leaf':[1,2,3,4],\n",
    "    'simpleimputer__strategy': ['mean', 'median'], \n",
    "    'xgbclassifier__booster': randint(0, 350, 20), \n",
    "    'randomforestclassifier__max_depth': [5, 10, 15, 20,None], \n",
    "    'randomforestclassifier__max_features': uniform(0, 1)\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline, \n",
    "    param_distributions=param_distributions, \n",
    "    n_iter=60, \n",
    "    cv=5, \n",
    "    scoring='accuracy', \n",
    "    verbose=10, \n",
    "    return_train_score=True, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.8091540404040404\n",
      "Validation Accuracy 0.7857744107744108\n"
     ]
    }
   ],
   "source": [
    "#Option 9\n",
    "#0.8058922558922559\n",
    "# %%time\n",
    "\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    ce.TargetEncoder(),\n",
    "    \n",
    "    \n",
    "    SimpleImputer(strategy='median'), \n",
    "#     StandardScaler(), \n",
    "     XGBClassifier(eval_metric='rmse',eta=0.1,objective= 'reg:linear',\n",
    "                   max_depth=6, num_round=10000,gamma=0, subsample=0.55, colsample_bytree=0.55,\n",
    "                   n_monte_carlo=1,cv_n=5,test_rounds_fac=1.2,count_n=0,mc_test=True,\n",
    "                   n_jobs=-1)\n",
    "    #Max_features features to consider when looking for the best split\n",
    "    #oob_score yes or nor uses samples to estimate the generalization accuracy.\n",
    "    #min_samples_leaf minimum number of samples required to be at a leaf node.\n",
    ")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print('Train Accuracy', pipeline.score(X_train, y_train))\n",
    "print('Validation Accuracy', pipeline.score(X_val, y_val))\n",
    "# Predict on test\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    ce.TargetEncoder(),\n",
    "    SimpleImputer(), \n",
    "    XGBClassifier()\n",
    ")\n",
    "\n",
    "param_distributions = {\n",
    "    \n",
    "    'targetencoder__min_samples_leaf':[1,2,3,4],\n",
    "    'simpleimputer__strategy': ['mean', 'median'], \n",
    "    'xgbclassifier__eval_metric':['rmse','mae','logloss'],\n",
    "    'xgbclassifier__booster':['gbtree','gblinear','dart'],\n",
    "    'xgbclassifier__objective':['reg:linear','binary:logistic','multi:softmax','multi:softprob' ],\n",
    "    'xgbclassifier__max_depth': [3, 5, 6, 10,None], \n",
    "    'xgbclassifier__num_round': randint(100, 2000,10),\n",
    "    'xgbclassifier__gamma': uniform(0, 1),\n",
    "    'xgbclassifier__eta': uniform(0, 1),\n",
    "    'xgbclassifier__cv_n': [2,3,5],\n",
    "    'xgbclassifier__n_estimators':[150,500,1000,5000]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline, \n",
    "    param_distributions=param_distributions, \n",
    "    n_iter=50, \n",
    "    cv=5, \n",
    "    scoring='accuracy', \n",
    "    verbose=10, \n",
    "    return_train_score=True, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed: 23.1min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-232-32ec1d801f54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/unit2/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/unit2/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1467\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1468\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1469\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/unit2/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    665\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 667\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/unit2/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/unit2/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/unit2/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/unit2/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/unit2/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ce.OneHotEncoder(use_cat_names=True),\n",
    "    SimpleImputer(),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "param_distributions = {\n",
    "    'simpleimputer__strategy': ['mean','median'],\n",
    "    'randomforestclassifier__n_estimators': randint(100,500),\n",
    "    'randomforestclassifier__max_depth': [25,50,100,200,None],\n",
    "    'randomforestclassifier__max_features': uniform(0,1),\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=25,\n",
    "    cv=5,\n",
    "    verbose=10,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "search.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Filenames of your submissions you want to ensemble\n",
    "files = ['submission-Rob_Simple.csv', 'submission-ADA_Iterative.csv', 'submission-Q_Iterative.csv', 'submission-Tree_Iterative.csv']\n",
    "\n",
    "submissions = (pd.read_csv(file)[['status_group']] for file in files)\n",
    "ensemble = pd.concat(submissions, axis='columns')\n",
    "majority_vote = ensemble.mode(axis='columns')[0]\n",
    "\n",
    "submission = sample_submission.copy()\n",
    "submission['status_group'] = majority_vote\n",
    "submission.to_csv('my-ultimate-ensemble-submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment_kaggle_challenge_3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
